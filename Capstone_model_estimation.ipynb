{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "747inL6S5ust"
      },
      "outputs": [],
      "source": [
        "#import the necessary modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn import svm\n",
        "import xgboost as xgb\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#Mount Drive on Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "\n",
        "#load the data into Colab\n",
        "reddit_df = pd.read_csv('/content/drive/MyDrive/reduced_dfm_tfidf.csv')\n",
        "reddit_df = reddit_df.iloc[:, 1:]\n",
        "reddit_df = reddit_df.drop(['X', 'subreddit.1', 'utc_datetime_str'], axis=1)\n",
        "\n",
        "#extract the targets and the features from the dataset\n",
        "y1 = reddit_df[['left_right']]\n",
        "y2 = reddit_df[['auth_lib']]\n",
        "y = reddit_df[['left_right', 'auth_lib']]\n",
        "X = reddit_df.iloc[:, :-2]\n",
        "\n",
        "#estimate a linear regression model for reference\n",
        "lm = linear_model.LinearRegression()\n",
        "\n",
        "cv_results = cross_validate(lm, X, y1, cv=5, scoring='neg_mean_squared_error')\n",
        "mean_mse_lm1 = sum(cv_results['test_neg_mean_squared_error'])/len(cv_results['test_neg_mean_squared_error'])\n",
        "\n",
        "cv_results = cross_validate(lm, X, y2, cv=5, scoring=('r2', 'neg_mean_squared_error'))\n",
        "mean_mse_lm2 = sum(cv_results['test_neg_mean_squared_error'])/len(cv_results['test_neg_mean_squared_error'])\n",
        "\n",
        "print(mean_mse_lm1)\n",
        "print(mean_mse_lm2)\n",
        "\n",
        "def output_performance(model, grid):\n",
        "  \"\"\"Takes an initialized model and a parameter grid in the form of a dictionary as input,\n",
        "  estimates the model on both axes, performs a grid search across the specified parameters\n",
        "  and returns average loss for the two axes.\n",
        "  \"\"\"\n",
        "  g_search = GridSearchCV(model, grid, cv = 5, scoring='neg_mean_squared_error', verbose = 1, refit = False)\n",
        "\n",
        "  g_search.fit(X, y1)\n",
        "\n",
        "  model1_mse = g_search.cv_results_['mean_test_neg_mean_squared_error']\n",
        "  model1_mse = model1_mse.tolist()\n",
        "  mse_ind = model1_mse.index(max(model1_mse))\n",
        "\n",
        "  g_search.fit(X, y2)\n",
        "\n",
        "  model2_mse = g_search.cv_results_['mean_test_neg_mean_squared_error']\n",
        "  model2_mse = model2_mse.tolist()\n",
        "  mse_ind = model2_mse.index(max(model2_mse))\n",
        "\n",
        "  av_mse = (max(model2_mse) + max(model1_mse))/2\n",
        "\n",
        "  print(av_mse)\n",
        "  return av_mse\n",
        "\n",
        "#initilialize models\n",
        "ridge = linear_model.Ridge()\n",
        "lasso = linear_model.Lasso()\n",
        "elasticnet = linear_model.ElasticNet(random_state = 42)\n",
        "tree = DecisionTreeRegressor(random_state = 42)\n",
        "rf = RandomForestRegressor(random_state = 42)\n",
        "gbr = GradientBoostingRegressor(random_state = 42)\n",
        "xgboost = = xgb.XGBRegressor()\n",
        "svr = svm.SVR()\n",
        "\n",
        "#define parameter grids for the corresponding models\n",
        "lasso_ridge_grid = {'alpha':[0.001, 0.01, 0.1, 0.2, 0.5, 0.9, 1, 5, 10, 20]}\n",
        "\n",
        "elasticnet_grid = {'alpha':[0.0001, 0.001, 0.01, 0.1, 0.2, 0.5],\n",
        "                   'l1_ratio':[0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 0.7, 0.9, 1],\n",
        "                   \"tol\":[0.0001]}\n",
        "\n",
        "tree_grid = {'max_depth': [4, 6, 8, 10, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150]}\n",
        "\n",
        "rf_grid = {'max_depth': [4, 6, 8, 10, 12],\n",
        "           'n_estimators': [50, 60, 70, 80, 90, 100, 120]}\n",
        "\n",
        "gbr_grid = {'max_depth': [4, 6, 8, 10, 12],\n",
        "            'n_estimators': [50, 60, 70, 80, 90, 100, 120]}\n",
        "\n",
        "xgb_grid = {'n_estimators': [50, 60, 70, 80, 90, 100, 120],\n",
        "            'max_depth': [4, 6, 8, 10, 12],\n",
        "            'eta': [0.01, 0.05, 0.1, 0.3],\n",
        "            'subsample': [0.5, 0.8, 0.9, 1],\n",
        "            'colsample_bytree': [0.5, 0.8, 0.9, 1]}\n",
        "\n",
        "svr_grid = {'kernel' : ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
        "            'gamma' : ['scale', 'auto'],\n",
        "            'C' : [0.1, 0.5, 0.8, 1, 2, 5, 10],\n",
        "            'epsilon' : [0.01, 0.1, 0.3, 0.5]}\n",
        "\n",
        "#perform grid search and evaluate performance\n",
        "output_performance(ridge, lasso_ridge_grid)\n",
        "output_performance(lasso, lasso_ridge_grid)\n",
        "output_performance(elasticnet, elasticnet_grid)\n",
        "output_performance(tree, tree_grid)\n",
        "output_performance(rf, rf_grid)\n",
        "output_performance(gbr, gbr_grid)\n",
        "output_performance(xgboost, xgb_grid)\n",
        "output_performance(svr, svr_grid)\n",
        "\n",
        "#divide the data into train and test sets for neural network estimation\n",
        "random.seed(42)\n",
        "idx = random.sample(range(0, len(X) - 1), round(0.2*len(X)))\n",
        "\n",
        "test_X, train_X = X.iloc[idx,:], X.iloc[~X.index.isin(idx), :]\n",
        "test_X, train_X = test_X.values[:, :], train_X.values[:, :]\n",
        "\n",
        "test_y1, train_y1 = y1.iloc[idx,:], y1.iloc[~y1.index.isin(idx), :]\n",
        "test_y1, train_y1 = test_y1.values[:, :], train_y1.values[:, :]\n",
        "\n",
        "test_y2, train_y2 = y2.iloc[idx,:], y2.iloc[~y2.index.isin(idx), :]\n",
        "test_y2, train_y2 = test_y2.values[:, :], train_y2.values[:, :]\n",
        "\n",
        "test_y, train_y = y.iloc[idx,:], y.iloc[~y.index.isin(idx), :]\n",
        "test_y, train_y = test_y.values[:, :], train_y.values[:, :]\n",
        "\n",
        "def create_model(rate, lr, n_y):\n",
        "  \"\"\"Takes a dropout rate and a learning rate as decimals and a number of outputs as an integer,\n",
        "  compiles a model with the architecture outlined below, returns the model\n",
        "  \"\"\"\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  model.add(layers.Conv1D(32, 8, activation='relu', input_shape = (train_X.shape[1], 1), name = \"Convolution_layer_1\"))\n",
        "\n",
        "  model.add(layers.Dense(200, activation = \"relu\", name = \"Dense_layer_1\"))\n",
        "\n",
        "  model.add(layers.Dropout(rate, name = \"Dropout_layer_1\"))\n",
        "\n",
        "  model.add(layers.Conv1D(16, 4, activation='relu', name = \"Convolution_layer_2\"))\n",
        "\n",
        "  model.add(layers.Dense(100, activation = 'relu', name = \"Dense_layer_2\"))\n",
        "\n",
        "  model.add(layers.Dropout(rate, name = \"Dropout_layer_2\"))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "\n",
        "  model.add(layers.Dense(n_y, name = \"Output_layer\"))\n",
        "\n",
        "  #Define optimizer\n",
        "  opt = keras.optimizers.SGD(learning_rate = lr)\n",
        "\n",
        "  model.compile(loss = 'mean_squared_error', optimizer = opt)\n",
        "\n",
        "  return model\n",
        "\n",
        "def evaluate_model(train_x, train_y, test_x, test_y, rate, lr, eps, n_y):\n",
        "  \"\"\"Accepts train and test sets for the features and targets as dataframes; dropout rate, learning rate as decimals;\n",
        "  number of epochs and the number of targets as integers; then, fits the model and returns test MSE.\n",
        "  \"\"\"\n",
        "  #Define model\n",
        "  model = create_model(rate, lr, n_y)\n",
        "\n",
        "  model.fit(train_x, train_y, epochs = eps, validation_split = 0.1)\n",
        "\n",
        "  #Get model predictions\n",
        "  y_pred = model.predict(test_x)\n",
        "\n",
        "  #Calculate test MSE\n",
        "  mse = keras.losses.MeanSquaredError()\n",
        "  test_mse = mse(test_y, y_pred).numpy()\n",
        "\n",
        "  return test_mse\n",
        "\n",
        "#first, try to predict axes separately\n",
        "evaluate_model(train_X, train_y1, test_X, test_y1, 0.2, 0.01, 20, 1)\n",
        "evaluate_model(train_X, train_y2, test_X, test_y2, 0.2, 0.01, 20, 1)\n",
        "\n",
        "#predict axes together\n",
        "evaluate_model(train_X, train_y, test_X, test_y, 0.2, 0.01, 20, 2)\n",
        "\n",
        "#try feature selection and estimate models again\n",
        "corr_coefs = [np.corrcoef(X.iloc[:, i], y2.values.ravel())[0][1] for i in range(len(X.columns))]\n",
        "thr = 0.05\n",
        "ind = [i for i in range(len(corr_coefs)) if corr_coefs[i] >= thr or corr_coefs[i] <= -thr]\n",
        "selected_features = X.columns[ind]\n",
        "X = X[selected_features]"
      ]
    }
  ]
}